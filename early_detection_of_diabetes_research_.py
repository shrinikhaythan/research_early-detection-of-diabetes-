# -*- coding: utf-8 -*-
"""early detection of diabetes research .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CVhURIl8Z9uq3Ei2-YptbZfx9uk7U5eB
"""

import pandas as pd
dfl=pd.read_csv ("/content/diabetes_prediction_dataset.csv.zip")
df = dfl.iloc[:2000].copy()
df.shape

df.head(5)

df['smoking_history'].unique()

import numpy as np
df['smoking_history'] = df['smoking_history'].replace('No Info', np.nan)
df['smoking_history'].unique()

#apart from smoking everything is fine
df.isnull().sum()

dfann= df
dfcat=df
dfcat.head(6)
#creating 2 copies for ann and catboost

df['gender'].unique()

#convert gender and smoking history to one hot encoding
# One-hot encode gender
 # creates Gender_Male or Gender_Female column
dfann= pd.get_dummies(df, columns=['smoking_history','gender'], drop_first=True)
dfann.columns

dfcat= pd.get_dummies(df, columns=['smoking_history','gender'], drop_first=True)
dfcat.columns

#handle null values for ann using knn imputer  for ann
# Columns to include for KNN imputation (example)
columns_to_impute = ['age', 'hypertension', 'heart_disease', 'bmi', 'HbA1c_level',
       'blood_glucose_level',  'smoking_history_ever',
       'smoking_history_former', 'smoking_history_never',
       'smoking_history_not current', 'gender_Male']

from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5)
dfann[columns_to_impute] = imputer.fit_transform(dfann[columns_to_impute])
dfann.head (5)
#no null values

columns_to_impute = ['age', 'hypertension', 'heart_disease', 'bmi', 'HbA1c_level',
       'blood_glucose_level',  'smoking_history_ever',
       'smoking_history_former', 'smoking_history_never',
       'smoking_history_not current', 'gender_Male']

from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5)
dfcat[columns_to_impute] = imputer.fit_transform(dfcat[columns_to_impute])
dfcat.head (5)
#handling null values for catboost

#removing outliers for dfann
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
import pandas as pd

# Suppose df is your full DataFrame (including encoded categorical features)
# Identify your numeric columns (either manually or using select_dtypes)
numeric_cols = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']  # replace with your actual numeric columns
df_numeric = dfann[numeric_cols]

# Step 2: Isolation Forest
iso = IsolationForest(contamination=0.05, random_state=42)
iso_outliers = iso.fit_predict(df_numeric)

# Step 3: Local Outlier Factor
lof = LocalOutlierFactor(n_neighbors=100)
lof_outliers = lof.fit_predict(df_numeric)

# Step 4: Keep only rows not marked as outliers by either
mask = (iso_outliers == 1) & (lof_outliers == 1)

# Step 5: Apply mask to full DataFrame (including encoded categorical features)
df_clean = dfann[mask]  # no error
dfann1=df_clean
dfann1.shape
#do not assign to the same variable

#removing outliers for catboosst
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
import pandas as pd

# Suppose df is your full DataFrame (including encoded categorical features)
# Identify your numeric columns (either manually or using select_dtypes)
numeric_cols = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']  # replace with your actual numeric columns
df_numeric = dfcat[numeric_cols]

# Step 2: Isolation Forest
iso = IsolationForest(contamination=0.05, random_state=42)
iso_outliers = iso.fit_predict(df_numeric)

# Step 3: Local Outlier Factor
lof = LocalOutlierFactor(n_neighbors=100)
lof_outliers = lof.fit_predict(df_numeric)

# Step 4: Keep only rows not marked as outliers by either
mask = (iso_outliers == 1) & (lof_outliers == 1)

# Step 5: Apply mask to full DataFrame (including encoded categorical features)
df_clean = dfcat[mask]  # no error
dfcat1=df_clean
dfcat1.shape
#do not assign to the same variable

import numpy as np

numeric_cols = dfann1.select_dtypes(include=np.number)
zero_counts = (numeric_cols == 0).sum()
negative_counts = (numeric_cols < 0).sum()
print(zero_counts)
dfann1.columns
#no null or anomaly values,outliers removed
#dfann1 and dfcat1 are the final preprocessed dataframes so far which needs to be used

#start from z acore normalization of ann, catboost no need
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Example DataFrame (replace with your actual df)
# dfann1 = pd.read_csv('your_data.csv')

# List your numeric columns to scale
numeric_cols = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']  # replace with your numeric column names

# Initialize the scaler
scaler = StandardScaler()

# Apply Z-score normalization using .loc to avoid SettingWithCopyWarning
dfann1.loc[:, numeric_cols] = scaler.fit_transform(dfann1[numeric_cols])

# Check the result
print(dfann1.head())

'''#use correlation matrix for feature selection for ann
import matplotlib.pyplot as plt
import seaborn as sns

# Calculate correlation matrix for numeric columns only
corr_matrix = dfann1.corr()

# Plot heatmap of correlation matrix
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix of dfann1 Features')
plt.show()
columns_to_drop = ['smoking_history_not current']  # replace with your column names
dfann1 = dfann1.drop(columns=columns_to_drop)
dfann1.head(5)'''

'''columns_to_drop = [ 'smoking_history_never']  # replace with your column names
dfann1 = dfann1.drop(columns=columns_to_drop)
dfann1.head(5)'''

dfann1.head(5)

dfann1.shape
#dropped 3 columns as correlation score is very less for gender_other, smoking history never and smoking history not current

#to see the class imbalance
dfann1['diabetes'].value_counts()

'''dfcat1.shape
columns_to_drop = [ 'smoking_history_never','smoking_history_not current']  # replace with your column names
dfcat1 = dfcat1.drop(columns=columns_to_drop) #feature selection for catboost'''

!pip install optuna  #installing optuna

#hyper parameter finetuning using baysian optuna
from sklearn.model_selection import cross_val_score


import numpy as np
import pandas as pd
import optuna
from sklearn.model_selection import train_test_split
from imblearn.combine import SMOTEENN
from sklearn.metrics import accuracy_score, recall_score, f1_score,precision_score
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, PReLU, Dropout, Input
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

# Prepare the data
X = dfann1.drop('diabetes', axis=1).values
y = dfann1['diabetes'].values

scaler = StandardScaler()
X = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Apply SMOTEENN to training data
smote_enn = SMOTEENN(random_state=42)
X_train_res, y_train_res = smote_enn.fit_resample(X_train, y_train)

# Define Optuna objective
def objective(trial):
    neurons1 = trial.suggest_int("neurons1", 16, 256)
    neurons2 = trial.suggest_int("neurons2", 16, 128)
    learning_rate = trial.suggest_float("learning_rate", 1e-4, 1e-2, log=True)
    dropout_rate = trial.suggest_float("dropout_rate", 0.0, 0.7)
    batch_size = trial.suggest_int("batch_size", 16, 128)
    epochs = trial.suggest_int("epochs", 5, 50)

    model = Sequential()
    model.add(Input(shape=(X.shape[1],)))
    model.add(Dense(neurons1))
    model.add(PReLU())
    model.add(Dropout(dropout_rate))
    model.add(Dense(neurons2))
    model.add(PReLU())
    model.add(Dropout(dropout_rate))
    model.add(Dense(1, activation='sigmoid'))

    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='binary_crossentropy', metrics=['accuracy'])

    model.fit(X_train_res, y_train_res, epochs=epochs, batch_size=batch_size, verbose=0)

    y_pred = model.predict(X_test).flatten()
    y_pred_classes = (y_pred > 0.5).astype(int)

    acc = accuracy_score(y_test, y_pred_classes)
    recall = recall_score(y_test, y_pred_classes)
    f1 = f1_score(y_test, y_pred_classes)
    prec = precision_score(y_test, y_pred_classes)

    return acc # Or return a custom weighted score

# Run the study
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=60)

# Show best params
best_params = study.best_params
print("\nüéØ Best Hyperparameters from Optuna:")
for k, v in best_params.items():
    print(f"{k}: {v}")

# --- Final Model Training and Evaluation ---
neurons1 = best_params['neurons1']
neurons2 = best_params['neurons2']
learning_rate = best_params['learning_rate']
dropout_rate = best_params['dropout_rate']
batch_size = best_params['batch_size']
epochs = best_params['epochs']

final_model = Sequential()
final_model.add(Input(shape=(X.shape[1],)))
final_model.add(Dense(neurons1))
final_model.add(PReLU())
final_model.add(Dropout(dropout_rate))
final_model.add(Dense(neurons2))
final_model.add(PReLU())
final_model.add(Dropout(dropout_rate))
final_model.add(Dense(1, activation='sigmoid'))

final_model.compile(optimizer=Adam(learning_rate=learning_rate),
                    loss='binary_crossentropy', metrics=['accuracy'])

final_model.fit(X_train_res, y_train_res, epochs=epochs, batch_size=batch_size, verbose=0)

# Final evaluation
y_pred = final_model.predict(X_test).flatten()
y_pred_classes = (y_pred > 0.5).astype(int)

acc = accuracy_score(y_test, y_pred_classes)
rec = recall_score(y_test, y_pred_classes)
f1 = f1_score(y_test, y_pred_classes)
prec = precision_score(y_test, y_pred_classes)

print("\nüìä Final Evaluation on Test Set:")
print(f"Accuracy:  {acc:.4f}")
print(f"Recall:    {rec:.4f}")
print(f"F1 Score:  {f1:.4f}")
print(f"Precision: {prec:.4f}")

import numpy as np #final hyperparameter finetuning and testing for ann by optuna bayesian and smote-enn
import pandas as pd
import optuna
from sklearn.model_selection import train_test_split, KFold
from imblearn.combine import SMOTEENN
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, PReLU, Dropout, Input
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

# Assuming dfann1 already exists
X = dfann1.drop('diabetes', axis=1).values
y = dfann1['diabetes'].values


# Split once into train-test for final evaluation later
X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Objective function using 3-fold CV and SMOTEENN
def objective(trial):
    neurons1 = trial.suggest_int("neurons1", 16, 256)
    neurons2 = trial.suggest_int("neurons2", 16, 128)
    learning_rate = trial.suggest_float("learning_rate", 1e-4, 1e-2, log=True)
    dropout_rate = trial.suggest_float("dropout_rate", 0.0, 0.7)
    batch_size = trial.suggest_int("batch_size", 16, 128)
    epochs = trial.suggest_int("epochs", 5, 50)

    kf = KFold(n_splits=3, shuffle=True, random_state=42)
    acc_scores = []

    for train_idx, val_idx in kf.split(X_train_full):
        X_train_cv, X_val_cv = X_train_full[train_idx], X_train_full[val_idx]
        y_train_cv, y_val_cv = y_train_full[train_idx], y_train_full[val_idx]

        # Apply SMOTEENN to the training fold
        smote_enn = SMOTEENN(random_state=42)
        X_train_res, y_train_res = smote_enn.fit_resample(X_train_cv, y_train_cv)

        # Define the ANN model
        model = Sequential()
        model.add(Input(shape=(X.shape[1],)))
        model.add(Dense(neurons1))
        model.add(PReLU())
        model.add(Dropout(dropout_rate))
        model.add(Dense(neurons2))
        model.add(PReLU())
        model.add(Dropout(dropout_rate))
        model.add(Dense(1, activation='sigmoid'))

        model.compile(optimizer=Adam(learning_rate=learning_rate),
                      loss='binary_crossentropy', metrics=['accuracy'])

        model.fit(X_train_res, y_train_res, epochs=epochs, batch_size=batch_size, verbose=0)

        y_pred = model.predict(X_val_cv).flatten()
        y_pred_classes = (y_pred > 0.5).astype(int)

        acc = accuracy_score(y_val_cv, y_pred_classes)
        acc_scores.append(acc)

    return np.mean(acc_scores)  # Optimize for mean accuracy across 3 folds

# Run Optuna Study
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)

# Best hyperparameters
best_params = study.best_params
print("\n‚úÖ Best Hyperparameters:")
for k, v in best_params.items():
    print(f"{k}: {v}")

# Final training on full training set and evaluation on held-out test set
neurons1 = best_params['neurons1']
neurons2 = best_params['neurons2']
learning_rate = best_params['learning_rate']
dropout_rate = best_params['dropout_rate']
batch_size = best_params['batch_size']
epochs = best_params['epochs']
neurons1 = best_params['neurons1']
neurons2 = best_params['neurons2']
learning_rate = best_params['learning_rate']
dropout_rate = best_params['dropout_rate']
batch_size = best_params['batch_size']
epochs = best_params['epochs']


# Balance the full training data
smote_enn = SMOTEENN(random_state=42)
X_train_res, y_train_res = smote_enn.fit_resample(X_train_full, y_train_full)

# Final model
final_model = Sequential()
final_model.add(Input(shape=(X.shape[1],)))
final_model.add(Dense(neurons1))
final_model.add(PReLU())
final_model.add(Dropout(dropout_rate))
final_model.add(Dense(neurons2))
final_model.add(PReLU())
final_model.add(Dropout(dropout_rate))
final_model.add(Dense(1, activation='sigmoid'))

final_model.compile(optimizer=Adam(learning_rate=learning_rate),
                    loss='binary_crossentropy', metrics=['accuracy'])

final_model.fit(X_train_res, y_train_res, epochs=epochs, batch_size=batch_size, verbose=0)

# Final evaluation
y_pred = final_model.predict(X_test).flatten()
y_pred_classes = (y_pred > 0.5).astype(int)

acc = accuracy_score(y_test, y_pred_classes)
prec = precision_score(y_test, y_pred_classes)
rec = recall_score(y_test, y_pred_classes)
f1 = f1_score(y_test, y_pred_classes)

print("\nüìä Final Test Set Evaluation:")
print(f"Accuracy:  {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall:    {rec:.4f}")
print(f"F1 Score:  {f1:.4f}")

final_model.save("my_ann_model smote final+enn.keras")

!pip install -U imbalanced-learn

import numpy as np  #hyper arameter fine tuning with smote tomek +enn
import pandas as pd
import optuna
from sklearn.model_selection import train_test_split, KFold
from imblearn.combine import SMOTETomek
from imblearn.under_sampling import EditedNearestNeighbours
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, PReLU, Dropout, Input
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

# Assuming dfann1 already exists
X = dfann1.drop('diabetes', axis=1).values
y = dfann1['diabetes'].values


# Hold-out split
X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Optuna objective
def objective(trial):
    neurons1 = trial.suggest_int("neurons1", 16, 128)
    neurons2 = trial.suggest_int("neurons2", 16, 256)
    learning_rate = trial.suggest_float("learning_rate", 1e-4, 1e-2, log=True)
    dropout_rate = trial.suggest_float("dropout_rate", 0.0, 0.7)
    batch_size = trial.suggest_int("batch_size", 16, 128)
    epochs = trial.suggest_int("epochs", 5, 50)

    kf = KFold(n_splits=3, shuffle=True, random_state=42)
    acc_scores = []

    for train_idx, val_idx in kf.split(X_train_full):
        X_train_cv, X_val_cv = X_train_full[train_idx], X_train_full[val_idx]
        y_train_cv, y_val_cv = y_train_full[train_idx], y_train_full[val_idx]


        smt = SMOTETomek(random_state=42)
        X_train_res, y_train_res = smt.fit_resample(X_train_cv, y_train_cv)

        model = Sequential()
        model.add(Input(shape=(X.shape[1],)))
        model.add(Dense(neurons1))
        model.add(PReLU())
        model.add(Dropout(dropout_rate))
        model.add(Dense(neurons2))
        model.add(PReLU())
        model.add(Dropout(dropout_rate))
        model.add(Dense(1, activation='sigmoid'))

        model.compile(optimizer=Adam(learning_rate=learning_rate),
                      loss='binary_crossentropy', metrics=['accuracy'])

        model.fit(X_train_res, y_train_res, epochs=epochs, batch_size=batch_size, verbose=0)

        y_pred = model.predict(X_val_cv).flatten()
        y_pred_classes = (y_pred > 0.4).astype(int)

        acc = accuracy_score(y_val_cv, y_pred_classes)
        acc_scores.append(acc)

    return np.mean(acc_scores)

# Run the study
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)

# Extract best params
best_params = study.best_params
print("\n‚úÖ Best Hyperparameters:")
for k, v in best_params.items():
    print(f"{k}: {v}")

# Final training + test evaluation
'''neurons1 = best_params['neurons1']
neurons2 = best_params['neurons2']
learning_rate = best_params['learning_rate']
dropout_rate = best_params['dropout_rate']
batch_size = best_params['batch_size']
epochs = best_params['epochs']

smt = SMOTETomek(random_state=42)
X_train_res, y_train_res = smt.fit_resample(X_train_full, y_train_full)

final_model = Sequential()
final_model.add(Input(shape=(X.shape[1],)))
final_model.add(Dense(neurons1))
final_model.add(PReLU())
final_model.add(Dropout(dropout_rate))
final_model.add(Dense(neurons2))
final_model.add(PReLU())
final_model.add(Dropout(dropout_rate))
final_model.add(Dense(1, activation='sigmoid'))

final_model.compile(optimizer=Adam(learning_rate=learning_rate),
                    loss='binary_crossentropy', metrics=['accuracy'])

final_model.fit(X_train_res, y_train_res, epochs=epochs, batch_size=batch_size, verbose=0)

# Final evaluation
y_pred = final_model.predict(X_test).flatten()
y_pred_classes = (y_pred > 0.5).astype(int)

acc = accuracy_score(y_test, y_pred_classes)
prec = precision_score(y_test, y_pred_classes)
rec = recall_score(y_test, y_pred_classes)
f1 = f1_score(y_test, y_pred_classes)

print("\nüìä Final Test Set Evaluation:")
print(f"Accuracy:  {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall:    {rec:.4f}")
print(f"F1 Score:  {f1:.4f}")'''

# Final training + test evaluation
neurons1 = best_params['neurons1']
neurons2 = best_params['neurons2']
learning_rate = best_params['learning_rate']
dropout_rate = best_params['dropout_rate']
batch_size = best_params['batch_size']
epochs = best_params['epochs']

smt = SMOTETomek(random_state=42)
X_train_res, y_train_res = smt.fit_resample(X_train_full, y_train_full)

final_model = Sequential()
final_model.add(Input(shape=(X.shape[1],)))
final_model.add(Dense(neurons1))
final_model.add(PReLU())
final_model.add(Dropout(dropout_rate))
final_model.add(Dense(neurons2))
final_model.add(PReLU())
final_model.add(Dropout(dropout_rate))
final_model.add(Dense(1, activation='sigmoid'))

final_model.compile(optimizer=Adam(learning_rate=learning_rate),
                    loss='binary_crossentropy', metrics=['accuracy'])

final_model.fit(X_train_res, y_train_res, epochs=epochs, batch_size=batch_size, verbose=0)

# Final evaluation
y_pred = final_model.predict(X_test).flatten()
y_pred_classes = (y_pred > 0.5).astype(int)

acc = accuracy_score(y_test, y_pred_classes)
prec = precision_score(y_test, y_pred_classes)
rec = recall_score(y_test, y_pred_classes)
f1 = f1_score(y_test, y_pred_classes)

print("\nüìä Final Test Set Evaluation:")
print(f"Accuracy:  {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall:    {rec:.4f}")
print(f"F1 Score:  {f1:.4f}")
final_model.save("my_ann_model smotetomk+enn final.keras")  # Saves in the native Keras format
   # Saves in HDF5 format
   #so far , ann with smotetomek is the better model ..then compared to smote+enn

# hyper parameter fine tuning and training catboost with smote +enn
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import accuracy_score
from imblearn.combine import SMOTETomek
from imblearn.under_sampling import EditedNearestNeighbours
import optuna
import numpy as np
import pandas as pd
import numpy as np  #hyper arameter fine tuning with smote tomek +enn
import pandas as pd
import optuna
from sklearn.model_selection import train_test_split, KFold
from imblearn.combine import SMOTEENN
from imblearn.combine import SMOTETomek
from imblearn.under_sampling import EditedNearestNeighbours
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, PReLU, Dropout, Input
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

df = dfcat1.copy()
X = df.drop("diabetes", axis=1)
y = df["diabetes"]
X_train1, X_test, y_train1, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def objective(trial):
    params = {
        "iterations": trial.suggest_int("iterations", 100, 300),
        "depth": trial.suggest_int("depth", 4, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1, 10),
        "loss_function": "Logloss",
        "verbose": 0,
        "random_seed": 42,
    }

    kf = KFold(n_splits=3, shuffle=True, random_state=42)
    acc_scores = []

    for train_idx, val_idx in kf.split(X_train1):
        X_train, X_val = X_train1.iloc[train_idx], X_train1.iloc[val_idx]
        y_train, y_val = y_train1.iloc[train_idx], y_train1.iloc[val_idx]

        # ‚úÖ Fill NaNs for SMOTETomek + ENN (not for CatBoost)


        # SMOTETomek + ENN (works only on numerical data, so this trick is needed)
        smote_enn = SMOTEENN(random_state=42)




        X_res, y_res = smote_enn.fit_resample(X_train, y_train)


        # Convert resampled back to DataFrame
        X_res = pd.DataFrame(X_res, columns=X.columns)

        model = CatBoostClassifier(**params)
        model.fit(X_res, y_res)

        y_pred = model.predict(X_val)
        acc = accuracy_score(y_val, y_pred)
        acc_scores.append(acc)

    return np.mean(acc_scores)

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=30)

print("‚úÖ Best hyperparameters:")
print(study.best_params)


# üîÅ Run Optuna Study
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)

# üéØ Best Hyperparameters
print("‚úÖ Best Hyperparameters:")
for key, value in study.best_params.items():
    print(f"{key}: {value}")

# ‚è±Ô∏è Optional: Train final model with best params
best_params = study.best_params
best_params.update({
    "loss_function": "Logloss",
    "verbose": 0,
    "random_seed": 42
})

# Final model
final_model = CatBoostClassifier(**best_params)
smote_enn = SMOTEENN(random_state=42)

X_res, y_res = smote_enn.fit_resample(X_train1,y_train1 )

final_model.fit(X_res, y_res)
y_pred = final_model.predict(X_test)

# üìà Evaluation Metrics
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("\nüìä Evaluation on Test Set:")
print(f"Accuracy : {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall   : {rec:.4f}")
print(f"F1 Score : {f1:.4f}")

# üíæ Save the model
final_model.save_model("catboost_diabetes_model.cbm")
print("\n‚úÖ Model saved as 'catboost_diabetes_model.cbm'")

dfcat1.isnull().sum()

!pip install -U catboost

from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import accuracy_score
from imblearn.combine import SMOTETomek
from imblearn.under_sampling import EditedNearestNeighbours
import optuna
import numpy as np
import pandas as pd
import numpy as np  #hyper arameter fine tuning with smote tomek +enn
import pandas as pd
import optuna
from sklearn.model_selection import train_test_split, KFold
from imblearn.combine import SMOTETomek
from imblearn.under_sampling import EditedNearestNeighbours
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, PReLU, Dropout, Input
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

df = dfcat1.copy()
X = df.drop("diabetes", axis=1)
y = df["diabetes"]
X_train1, X_test, y_train1, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def objective(trial):
    params = {
        "iterations": trial.suggest_int("iterations", 100, 300),
        "depth": trial.suggest_int("depth", 4, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1, 10),
        "loss_function": "Logloss",
        "verbose": 0,
        "random_seed": 42,
    }

    kf = KFold(n_splits=3, shuffle=True, random_state=42)
    acc_scores = []

    for train_idx, val_idx in kf.split(X_train1):
        X_train, X_val = X_train1.iloc[train_idx], X_train1.iloc[val_idx]
        y_train, y_val = y_train1.iloc[train_idx], y_train1.iloc[val_idx]

        # ‚úÖ Fill NaNs for SMOTETomek + ENN (not for CatBoost)


        # SMOTETomek + ENN (works only on numerical data, so this trick is needed)
        smt = SMOTETomek(random_state=42)
        enn = EditedNearestNeighbours()



        X_res, y_res = smt.fit_resample(X_train, y_train)
        X_res, y_res = enn.fit_resample(X_res, y_res)

        # Convert resampled back to DataFrame
        X_res = pd.DataFrame(X_res, columns=X.columns)

        model = CatBoostClassifier(**params)
        model.fit(X_res, y_res)

        y_pred = model.predict(X_val)
        acc = accuracy_score(y_val, y_pred)
        acc_scores.append(acc)

    return np.mean(acc_scores)

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=30)

print("‚úÖ Best hyperparameters:")
print(study.best_params)


# üîÅ Run Optuna Study
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)

# üéØ Best Hyperparameters
print("‚úÖ Best Hyperparameters:")
for key, value in study.best_params.items():
    print(f"{key}: {value}")

# ‚è±Ô∏è Optional: Train final model with best params
best_params = study.best_params
best_params.update({
    "loss_function": "Logloss",
    "verbose": 0,
    "random_seed": 42
})

# Final model
final_model = CatBoostClassifier(**best_params)
smt = SMOTETomek(random_state=42)
enn = EditedNearestNeighbours()

X_res, y_res = smt.fit_resample(X_train1,y_train1 )
X_res, y_res = enn.fit_resample(X_res, y_res)

final_model.fit(X_res, y_res)
y_pred = final_model.predict(X_test)

# üìà Evaluation Metrics
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("\nüìä Evaluation on Test Set:")
print(f"Accuracy : {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall   : {rec:.4f}")
print(f"F1 Score : {f1:.4f}")

# üíæ Save the model
final_model.save_model("catboost_diabetes_model.cbm")
print("\n‚úÖ Model saved as 'catboost_diabetes_model.cbm'")

import numpy as np #ensembling of model
import pandas as pd
from catboost import CatBoostClassifier
from keras.models import load_model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# üéØ Load dataset
df = dfcat1.copy()
X = df.drop("diabetes", axis=1)
y = df["diabetes"]

# üß™ Split into train/test (no resampling here)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# üíæ Load pre-trained models
catboost_model = CatBoostClassifier()
catboost_model.load_model("/content/catboost_diabetes_model.cbm")

ann_model = load_model("/content/my_ann_model smotetomk+enn final.keras")

# üîÑ Normalize test data for ANN only
scaler = MinMaxScaler()
X_train_ann = scaler.fit_transform(X_train)
X_test_ann = scaler.transform(X_test)

# ‚öôÔ∏è Predict probabilities
catboost_probs = catboost_model.predict_proba(X_test)[:, 1]
ann_probs = ann_model.predict(X_test_ann).flatten()

# ‚öñÔ∏è Weighted Voting
weight_catboost = 0.7
weight_ann = 0.3

combined_probs = weight_catboost * catboost_probs + weight_ann * ann_probs
y_pred_ensemble = (combined_probs >= 0.5).astype(int)

# üìä Evaluation
print("üìä Ensemble Model Evaluation:")
print(f"Accuracy : {accuracy_score(y_test, y_pred_ensemble):.4f}")
print(f"Precision: {precision_score(y_test, y_pred_ensemble):.4f}")
print(f"Recall   : {recall_score(y_test, y_pred_ensemble):.4f}")
print(f"F1 Score : {f1_score(y_test, y_pred_ensemble):.4f}")

dfann1.shape

dfcat1.shape

#ensembling of hyper parameter fine tuning
import numpy as np
import pandas as pd
from catboost import CatBoostClassifier
from keras.models import load_model
import optuna
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split

# Load data
df = dfcat1.copy()
X = df.drop("diabetes", axis=1)
y = df["diabetes"]

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Load models and scaler
catboost_model = CatBoostClassifier()
catboost_model.load_model("/content/catboost_diabetes_model (1).cbm")
ann_model = load_model("/content/my_ann_model smotetomk+enn final.keras")

# Prepare normalized data for ANN
from sklearn.preprocessing import StandardScaler

# Z-score normalization
scaler = StandardScaler()
X_test_ann = scaler.fit_transform(X_test)
# Predict probabilities once (used in every trial)
catboost_probs = catboost_model.predict_proba(X_test)[:, 1]
ann_probs = ann_model.predict(X_test_ann).flatten()

# Define Optuna objective
def objective(trial):
    w_cb = trial.suggest_float("catboost_weight", 0.0, 1.0)
    w_ann = 1.0 - w_cb

    combined_probs = w_cb * catboost_probs + w_ann * ann_probs
    y_pred = (combined_probs >= 0.5).astype(int)

    return accuracy_score(y_test, y_pred)

# Run Optuna study
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)

# Get best weights
best_w_cb = study.best_params["catboost_weight"]
best_w_ann = 1 - best_w_cb

print(f"‚úÖ Best Weights ‚Äî CatBoost: {best_w_cb:.2f}, ANN: {best_w_ann:.2f}")

# Final prediction using best weights
final_probs = best_w_cb * catboost_probs + best_w_ann * ann_probs
final_preds = (final_probs >= 0.5).astype(int)

# Evaluation metrics
acc = accuracy_score(y_test, final_preds)
prec = precision_score(y_test, final_preds)
rec = recall_score(y_test, final_preds)
f1 = f1_score(y_test, final_preds)

print("\nüìä Final Ensemble Model Evaluation:")
print(f"Accuracy : {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall   : {rec:.4f}")
print(f"F1 Score : {f1:.4f}")

#apply catboost using smote enn

from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import accuracy_score
from imblearn.combine import SMOTETomek
from imblearn.under_sampling import EditedNearestNeighbours
import optuna
import numpy as np
import pandas as pd
import numpy as np  #hyper arameter fine tuning with smote tomek +enn
import pandas as pd
import optuna
from sklearn.model_selection import train_test_split, KFold
from imblearn.combine import SMOTETomek
from imblearn.under_sampling import EditedNearestNeighbours
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, PReLU, Dropout, Input
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

df = dfcat1.copy()
X = df.drop("diabetes", axis=1)
y = df["diabetes"]
X_train1, X_test, y_train1, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def objective(trial):
    params = {
        "iterations": trial.suggest_int("iterations", 100, 300),
        "depth": trial.suggest_int("depth", 4, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1, 10),
        "loss_function": "Logloss",
        "verbose": 0,
        "random_seed": 42,
    }

    kf = KFold(n_splits=3, shuffle=True, random_state=42)
    acc_scores = []

    for train_idx, val_idx in kf.split(X_train1):
        X_train, X_val = X_train1.iloc[train_idx], X_train1.iloc[val_idx]
        y_train, y_val = y_train1.iloc[train_idx], y_train1.iloc[val_idx]

        # ‚úÖ Fill NaNs for SMOTETomek + ENN (not for CatBoost)


        # SMOTETomek + ENN (works only on numerical data, so this trick is needed)
        smote_enn = SMOTEENN(random_state=42)




        X_res, y_res = smote_enn.fit_resample(X_train, y_train)
        X

        # Convert resampled back to DataFrame
        X_res = pd.DataFrame(X_res, columns=X.columns)

        model = CatBoostClassifier(**params)
        model.fit(X_res, y_res)

        y_pred = model.predict(X_val)
        acc = accuracy_score(y_val, y_pred)
        acc_scores.append(acc)

    return np.mean(acc_scores)

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=30)

print("‚úÖ Best hyperparameters:")
print(study.best_params)


# üîÅ Run Optuna Study
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)

# üéØ Best Hyperparameters
print("‚úÖ Best Hyperparameters:")
for key, value in study.best_params.items():
    print(f"{key}: {value}")

# ‚è±Ô∏è Optional: Train final model with best params
best_params = study.best_params
best_params.update({
    "loss_function": "Logloss",
    "verbose": 0,
    "random_seed": 42
})

# Final model
final_model = CatBoostClassifier(**best_params)
smt = SMOTETomek(random_state=42)
enn = EditedNearestNeighbours()

X_res, y_res = smt.fit_resample(X_train1,y_train1 )
X_res, y_res = enn.fit_resample(X_res, y_res)

final_model.fit(X_res, y_res)
y_pred = final_model.predict(X_test)

# üìà Evaluation Metrics
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("\nüìä Evaluation on Test Set:")
print(f"Accuracy : {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall   : {rec:.4f}")
print(f"F1 Score : {f1:.4f}")

# üíæ Save the model
final_model.save_model("catboost_diabetes_model.cbm")
print("\n‚úÖ Model saved as 'catboost_diabetes_model.cbm'")

import matplotlib.pyplot as plt
import numpy as np   #TABLE 1

# Metrics
metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score','MCC','NPV']
# Models
model1 = [0.8989, 0.8562, 0.8452, 0.8507,0.8061,0.9333]
model2 = [0.9389, 0.9049, 0.9034, 0.9041,0.8798,0.9622]

x = np.arange(len(metrics))  # [0,1,2,3]
width = 0.35

plt.figure(figsize=(8,5))
plt.bar(x - width/2, model1, width, label='ANN (SMOTE+ENN)')
plt.bar(x + width/2, model2, width, label='ANN (SMOTETomek+ENN)')

plt.xticks(x, metrics)
plt.ylim(0.8, 1.0)
plt.ylabel('Score')
plt.title('ANN Model Comparison Across Metrics')
plt.legend()
plt.tight_layout()
plt.show()

# Models
model1 = [0.9282, 0.9248, 0.9346, 0.9297,0.8715,0.9607]
model2 = [0.9495, 0.9792, 0.9599, 0.9695,0.9212,0.9791]

x = np.arange(len(metrics))
width = 0.35

plt.figure(figsize=(8,5))
plt.bar(x - width/2, model1, width, label='CatBoost (SMOTE+ENN)')
plt.bar(x + width/2, model2, width, label='CatBoost (SMOTETomek+ENN)')

plt.xticks(x, metrics)
plt.ylim(0.8, 1.0)
plt.ylabel('Score')
plt.title('CatBoost Model Comparison Across Metrics')
plt.legend()
plt.tight_layout()
plt.show()

# Models
model1 = [0.9389, 0.9049, 0.9034, 0.9041,0.8798,0.9622]
model2 = [0.9495, 0.9792, 0.9599, 0.9695,0.9212,0.9791]
model3 = [0.9894, 1.0000, 0.9800, 0.9899,0.9797,0.9895]

x = np.arange(len(metrics))
width = 0.25

plt.figure(figsize=(10,5))
plt.bar(x - width, model1, width, label='ANN (SMOTETomek+ENN)')
plt.bar(x, model2, width, label='CatBoost (SMOTETomek+ENN)')
plt.bar(x + width, model3, width, label='ANN+CatBoost (Both with SMOTETomek+ENN)')

plt.xticks(x, metrics)
plt.ylim(0.85, 1.05)
plt.ylabel('Score')
plt.title('Final Ensemble vs Individual Models')
plt.legend()
plt.tight_layout()
plt.show()

#generating confusion matrix
from sympy import symbols, Eq, solve

# Given metrics
precision = 0.8562     # Replace with your value
recall = 0.8452       # Replace with your value
accuracy = 0.8989     # Replace with your value
f1_score = 0.8507    # Replace with your value
total = 400       # 20% of 1 lakh

# Define confusion matrix components
TP, FP, FN, TN = symbols('TP FP FN TN', integer=True)

# Equations based on metric definitions
eq1 = Eq(precision, TP / (TP + FP))
eq2 = Eq(recall, TP / (TP + FN))
eq3 = Eq(accuracy, (TP + TN) / total)
eq4 = Eq(total, TP + TN + FP + FN)

# Solve the system
solution = solve((eq1, eq2, eq3, eq4), (TP, FP, FN, TN), dict=True)

# Display result
if solution:
    print("Estimated Confusion Matrix (TP, FP, FN, TN):")
    for sol in solution:
        print({k: int(sol[k]) for k in sol})
else:
    print("No valid integer solution found. Try slightly adjusting the input metrics.")